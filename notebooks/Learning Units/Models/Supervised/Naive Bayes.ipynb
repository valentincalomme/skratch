{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "## About the model\n",
    "\n",
    "Naive Bayes is a technique used for classification that has been studied since the 1950s. It was first used for text categorization, predicting whether a text is of one type or another, spam or not spam, English or German, for instance. It is still used to this day as a benchmark for this class of tasks. Of course, it can be used for any classification task.\n",
    "\n",
    "This is one of the first papers introducing Bayesian techniques, including Naive Bayes, if you're curious: https://www.cs.utexas.edu/~jsinapov/teaching/cs378/readings/W2/Minsky60steps.pdf\n",
    "\n",
    "## Pros\n",
    "\n",
    "- Highly scalable\n",
    "- Easy to implement\n",
    "- Doesn't require a lot of data\n",
    "- Easily handles missing data and noise\n",
    "\n",
    "## Cons\n",
    "\n",
    "- Bad estimators of class probabilities\n",
    "- Assumptions made often do not hold\n",
    "- Simplistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Naive Bayes classifiers are generative models. This means that they will try to learn how instances of a certain class are generated by looking at what they inherently have in common.\n",
    "\n",
    "Let's illustrate this with an example. Let's say that we are given a dataset containing information about weight, height, and sex of people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at this data, we can infer that males seem to be taller and heavier than females. Therefore, if we had to generate new male data points, we would be more likely to generate people measuring 1m80 and weighting 85kg rather than people measuring 1m60 and weighting 60kg.\n",
    "\n",
    "Therefore, if we encounter a person measuring 1m92 and weighting 102kg, we would classify them as males because we would be more likely to generate such a person as a male rather than a female.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "To represent this concept mathematically, we make use of Bayes' Theorem. Which is represented by the following formula:\n",
    "\n",
    "$$P(Class \\mid Features) = \\frac{P(Class) P(Features \\mid Class)}{P(Features)}$$\n",
    "\n",
    "The theorem gives us a way to calculate the probability a class (male or female) given some features (height and weight). Therefore, to make a prediction, we will need to calculate the probability for both classes and pick the class with the highest one.\n",
    "\n",
    "If we are trying to classify a person measuring 1m80 and weighting 80kg, Bayes' Formula takes the following form:\n",
    "\n",
    "$$P(Gender \\mid Height=180, Weight = 60) = \\frac{P(Gender) \\space P(Height=180, Weight = 60 \\mid Gender)}{P(Height=180, Weight = 60)}$$\n",
    "\n",
    "Which can be read in statistical term as the following:\n",
    "\n",
    "$$Posterior = \\frac{Prior \\times Likelihood}{Evidence}$$\n",
    "\n",
    ">### Prior\n",
    "\n",
    ">The prior represents the belief that an instance is of a certain class \"prior\" to knowing its features. This is equivalent >to guessing whether someone is male or female without knowing anything about them.\n",
    "\n",
    ">### Likelihood\n",
    "\n",
    ">The likelihood represents how \"likely\" certain features are to appear given a class. For instance, it is more \"likely\" that a man is 1m90 rather than 1m50.\n",
    "\n",
    ">### Evidence\n",
    "\n",
    ">The evidence represents the belief that the features, which we use as \"evidence\" to infer the class, are correct. For instance, using the fact that a man is 1m80 and 80kg as evidence is stronger than using the fact that a male is 2m15 and 45kg, as it is much less probable to be true.\n",
    "\n",
    ">### Posterior\n",
    "\n",
    ">The posterior probability represents the belief that an event will occur \"after\" taking the features into account. For instance, how likely it is that someone is male, \"after\" we've considered that they measure 1m80 and weight 80kg.\n",
    "\n",
    "\n",
    "To simplify the formula, we can ignore the evidence as it is independent of the class. This means that we are now calculating the **joint probability**, which is proportional to the **posterior probability**. We can also make use of the strong assumption of independence, which is why these classifiers are called \"Naïve\". What does this mean in practice? It means that our formula now looks like this:\n",
    "\n",
    "$$P(Gender \\mid Height=180, Weight = 60) \\propto P(Gender) \\space P(Height=180 \\mid Gender) \\space P(Weight = 60 \\mid Gender)$$\n",
    "\n",
    "The strong assumption of independence simplified the likelihood by assuming that gender and height are independent of one another. It means that now, to calculate the posterior, and ultimately being able to classify an instance, we need to calculate these probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from \"skratch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will show how to implement a Naive Bayes classifier from scratch. We first start with a `predict` function. This function will compute the joint probability for each class as defined in `_predict_joint_proba`, and return the class with the highest joint probability.\n",
    "\n",
    "As described above, the joint probability is defined as $Prior \\times Likelihood$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NBClassifier:\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "\n",
    "        joint_probas = self._predict_joint_proba(X)\n",
    "        indices = np.argmax(joint_probas, axis=1)\n",
    "\n",
    "        return self.classes_[indices]\n",
    "\n",
    "    def _predict_joint_proba(self, X, y=None):\n",
    "\n",
    "        return np.array([[self._get_prior(c) * self._get_likelihood(sample, c) for c in self.classes_]\n",
    "                         for sample in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the joint probability is proportional but doesn't equal the exact probability, we can also have a function `predict_proba` computing the probabilities of each instance being of each class. This means that we divide the joint probability by the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict_proba(self, X, y=None):\n",
    "\n",
    "        joint_probas = self._predict_joint_proba(X, y)\n",
    "        evidence = np.array([[self._get_evidence(x)] for x in X])\n",
    "\n",
    "        return joint_probas / evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit the model, we need to learn how to compute the priors, the likelihood, and the evidence. And therefore, we also need to update them if we want to train on more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fit(self, X, y):\n",
    "\n",
    "        self.priors_ = self._fit_prior(y)\n",
    "        self.likelihood_ = self._fit_likelihood(X, y)\n",
    "        self.evidence_ = self._fit_evidence(X)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def update(self, X, y):\n",
    "\n",
    "        self.priors_ = self._update_priors(y)\n",
    "        self.likelihood_ = self._update_likelihood(X, y)\n",
    "        self.evidence_ = self._update_evidence(X)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the probabilities\n",
    "\n",
    "Now that we have shown the mechanics of fitting and predicting, we need to show how to deal with computing the required probabilities. Fitting and computing the priors is rather straightforward as it all relies on the frequencies of the classes in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _fit_prior(self, y):\n",
    "\n",
    "        self.classes_, self.priors_ = np.unique(y, return_counts=True)\n",
    "\n",
    "        return self.priors_\n",
    "\n",
    "    def _update_priors(self, y):\n",
    "\n",
    "        self.classes_, counts = np.unique(y, return_counts=True)\n",
    "        self.priors_ += counts\n",
    "\n",
    "        return self.priors_\n",
    "\n",
    "    def _get_prior(self, c):\n",
    "\n",
    "        return self.priors_[c] / np.sum(self.priors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the evidence and likelihood however, one must assume the probability distribution that the features follow. And there are plenty of them available. This is why we often refer to Naïve Bayes classifiers, plural, one for each distribution.\n",
    "\n",
    "There are three main types of Naïve Bayes classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes assumes that the features follow a normal distribution. It is a useful model when dealing with continuous data. It is often considered as the \"default\" Naïve Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/source/skratch/source\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from supervised.naive_bayes.nb_classifier import NBClassifier\n",
    "\n",
    "EPSILON = 1E-16  # offset to avoid \"divide by zero\" errors\n",
    "\n",
    "\n",
    "class GaussianNB(NBClassifier):\n",
    "\n",
    "    def _pdf(self, x, mean, std):\n",
    "\n",
    "        num = np.exp(-((x - mean)**2) / (EPSILON + 2 * std**2))\n",
    "        den = np.sqrt(2 * np.pi * std**2) + EPSILON\n",
    "\n",
    "        return num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the likelihood and the evidence comes down to estimating both parameters of the normal distribution, namely the mean and the standard deviation, for each feature. Fitting the likelihood is the same as fitting the evidence for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _fit_evidence(self, X):\n",
    "\n",
    "        feature_probas = []\n",
    "\n",
    "        for feature in X.T:\n",
    "            feature_probas.append(dict(mean=np.mean(feature),\n",
    "                                       n=len(feature),\n",
    "                                       std=np.std(feature, ddof=1)))\n",
    "\n",
    "        return np.array(feature_probas)\n",
    "\n",
    "    def _fit_likelihood(self, X, y):\n",
    "\n",
    "        likelihood_ = []\n",
    "\n",
    "        for c in self.classes_:\n",
    "            samples = X[y == c]\n",
    "\n",
    "            likelihood_.append(self._fit_evidence(samples))\n",
    "\n",
    "        return np.array(likelihood_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are given more data and we'd like to update the likelihood and evidence, we will need to update the mean and standard deviation. Luckily, we can do this without having to compute the mean and standard deviation from scratch. We can use the incremental mean and standard deviation formulae. These require that we keep track of how many instances the model was trained on, which is why we keep track of `n` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _update_evidence(self, X):\n",
    "\n",
    "        for i, feature in enumerate(X.T):\n",
    "            self.evidence_[i] = self._update_mean_std_n(feature, self.evidence_[i])\n",
    "\n",
    "        return self.evidence_\n",
    "\n",
    "    def _update_likelihood(self, X, y):\n",
    "\n",
    "        for c in self.classes_:\n",
    "            samples = X[y == c]\n",
    "\n",
    "            for i, feature in enumerate(samples.T):\n",
    "                self.likelihood_[c][i] = self._update_mean_std_n(feature, self.likelihood_[c][i])\n",
    "\n",
    "        return self.likelihood_\n",
    "\n",
    "    def _update_mean_std_n(self, feature, mean_std_n):\n",
    "\n",
    "        old_m = mean_std_n[\"mean\"]\n",
    "        old_std = mean_std_n[\"std\"]\n",
    "        old_n = mean_std_n[\"n\"]\n",
    "\n",
    "        n = old_n + len(feature)\n",
    "\n",
    "        m = (old_m * old_n + np.mean(feature) * n) / (old_n + n)\n",
    "\n",
    "        s = np.sqrt((old_n * (old_std**2 + (old_m - m)**2)\n",
    "                     + len(feature) * (np.var(feature)\n",
    "                                       + (np.mean(feature) - m)**2)\n",
    "                     ) / (old_n + len(feature)))\n",
    "\n",
    "        return dict(mean=m, std=std, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, calculating the likelihood and evidence comes down to computing each feature's probability density function and multiply them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_evidence(self, sample):\n",
    "\n",
    "        evidence = 1.0\n",
    "\n",
    "        for i, feature in enumerate(sample):\n",
    "\n",
    "            mean = self.evidence_[i][\"mean\"]\n",
    "            std = self.evidence_[i][\"std\"]\n",
    "\n",
    "            evidence *= self._pdf(feature, mean, std)\n",
    "\n",
    "        return evidence\n",
    "\n",
    "    def _get_likelihood(self, sample, c):\n",
    "\n",
    "        likelihood = 1.0\n",
    "\n",
    "        for i, feature in enumerate(sample):\n",
    "\n",
    "            mean = self.likelihood_[c][i][\"mean\"]\n",
    "            std = self.likelihood_[c][i][\"std\"]\n",
    "\n",
    "            likelihood *= self._pdf(feature, mean, std)\n",
    "\n",
    "        return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes\n",
    "\n",
    "Bernoulli Naive Bayes assumes that the features follow a Bernoulli distribution. Therefore, it also assumes that the features are binary. This classifier is useful as it takes the presence as well as the absence of features into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from supervised.naive_bayes.nb_classifier import NBClassifier\n",
    "\n",
    "\n",
    "class BernoulliNB(NBClassifier):\n",
    "\n",
    "    def _pdf(self, x, p):\n",
    "\n",
    "        return (1.0 - x) * (1.0 - p) + x * p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the evidence requires us to keep track of how many times a feature was positive, and how many times the feature occurred in total. Fitting the likelihood means that we need to calculate the evidence for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _fit_evidence(self, X):\n",
    "\n",
    "        feature_probas = [dict(count=np.sum(feature), n=len(feature)) for feature in X.T]\n",
    "\n",
    "        return np.array(feature_probas)\n",
    "\n",
    "    def _fit_likelihood(self, X, y):\n",
    "\n",
    "        likelihood_ = []\n",
    "\n",
    "        for c in self.classes_:\n",
    "            samples = X[y == c]\n",
    "\n",
    "            likelihood_.append(self._fit_evidence(samples))\n",
    "\n",
    "        return likelihood_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating therefore only requires us to update these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _update_evidence(self, X):\n",
    "\n",
    "        for i, feature in enumerate(X.T):\n",
    "\n",
    "            self.evidence[i][\"count\"] += np.sum(feature)\n",
    "            self.evidence[i][\"n\"] += len(feature)\n",
    "\n",
    "        return self.evidence_\n",
    "\n",
    "    def _update_likelihood(self, X, y):\n",
    "\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            samples = X[y == c]\n",
    "\n",
    "            for i, feature in enumerate(samples.T):\n",
    "\n",
    "                self.likelihood_[c][i][\"count\"] += np.sum(feature)\n",
    "                self.likelihood_[c][i][\"n\"] += len(feature)\n",
    "\n",
    "        return self.likelihood_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, calculating the likelihood and evidence comes down to computing each feature's probability density function and multiply them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_evidence(self, sample):\n",
    "\n",
    "        evidence = 1.0\n",
    "\n",
    "        for i, feature in enumerate(sample):\n",
    "\n",
    "            count = self.evidence_[i][\"count\"]\n",
    "            n = self.evidence_[i][\"n\"]\n",
    "\n",
    "            evidence *= self._pdf(x=feature, p=count / n)\n",
    "\n",
    "        return evidence\n",
    "\n",
    "    def _get_likelihood(self, sample, c):\n",
    "\n",
    "        likelihood = 1.0\n",
    "\n",
    "        for i, feature in enumerate(sample):\n",
    "\n",
    "            count = self.likelihood_[c][i][\"count\"]\n",
    "            n = self.likelihood_[c][i][\"n\"]\n",
    "\n",
    "            likelihood *= self._pdf(x=feature, p=count / n)\n",
    "\n",
    "        return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "Bernoulli Naive Bayes assumes that the features follow a Multinomial distribution. It is quite useful to classify documents as the frequencies of certain words can affect the type of the document.\n",
    "\n",
    "A good practice when dealing with categorical data is to use additive smoothing, also called Laplace smoothing. This is why we pass the `alpha` argument in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial as fact\n",
    "from collections import Counter\n",
    "\n",
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "\n",
    "from supervised.naive_bayes.nb_classifier import NBClassifier\n",
    "\n",
    "\n",
    "class MultinomialNB(NBClassifier):\n",
    "\n",
    "    def __init__(self, alpha=1.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _pdf(self, x, p):\n",
    "\n",
    "        f = fact(np.sum(x))\n",
    "\n",
    "        for P, X in zip(p, x):\n",
    "            f *= (P**X) / fact(X)\n",
    "\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the evidence comes to down to counting how many times each feature occured in the total, and the likelihood is similar to computing the evidence for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _fit_evidence(self, X):\n",
    "\n",
    "        evidence_ = np.sum(X, axis=0)\n",
    "\n",
    "        return evidence_\n",
    "\n",
    "    def _fit_likelihood(self, X, y):\n",
    "\n",
    "        likelihood_ = []\n",
    "\n",
    "        for c in self.classes_:\n",
    "            samples = X[y == c]\n",
    "\n",
    "            likelihood_.append(self._fit_evidence(samples))\n",
    "\n",
    "        return likelihood_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating then requires us to update these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _update_evidence(self, X):\n",
    "\n",
    "        self.evidence_ += np.sum(X, axis=0)\n",
    "\n",
    "        return self.evidence_\n",
    "\n",
    "    def _update_likelihood(self, X, y):\n",
    "\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            samples = X[y == c]\n",
    "\n",
    "            self.likelihood_[i] += np.sum(samples, axis=0)\n",
    "\n",
    "        return likelihood_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, calculating the likelihood and evidence comes down to computing each feature's probability density function and multiply them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_evidence(self, sample):\n",
    "\n",
    "        p = []\n",
    "\n",
    "        for i, feature in enumerate(sample):\n",
    "\n",
    "            x = self.evidence_[i]\n",
    "            N = np.sum(self.evidence_)\n",
    "            d = len(sample)\n",
    "            a = self.alpha\n",
    "\n",
    "            prob = (x + a) / (N + (a * d))\n",
    "\n",
    "            p.append(prob)\n",
    "\n",
    "        return self._pdf(sample, p)\n",
    "\n",
    "    def _get_likelihood(self, sample, c):\n",
    "\n",
    "        p = []\n",
    "\n",
    "        for i, feature in enumerate(sample):\n",
    "\n",
    "            x = self.likelihood_[c][i]\n",
    "            N = np.sum(self.likelihood_[c])\n",
    "            d = len(sample)\n",
    "            a = self.alpha\n",
    "\n",
    "            prob = (x + a) / (N + (a * d))\n",
    "\n",
    "            p.append(prob)\n",
    "\n",
    "        return self._pdf(sample, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "\n",
    "### Why is it called Naïve?\n",
    "\n",
    "Makes use of the strong assumption of independence\n",
    "\n",
    "### What probability distribution can be used?\n",
    "\n",
    "The algorithm gives no guarantee to find the best clusters every time. Indeed, based on the first random clusters, the algorithm might converge to a different solution. This is why, K-Means is typically run multiple times in order to increase the chances to not get stuck with a bad solution.\n",
    "\n",
    "### Can different features have different distributions?\n",
    "\n",
    "Yes\n",
    "\n",
    "### Is multinomial naive bayes with binary features the same as bernoulli naive bayes?\n",
    "\n",
    "No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful resources\n",
    "\n",
    "Wikipedia:\n",
    "\n",
    "   \n",
    "\n",
    "Tutorials:  \n",
    "\n",
    "\n",
    "Implementations:\n",
    "\n",
    "\n",
    "Videos:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
